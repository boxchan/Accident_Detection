{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be572b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_19308\\1867929233.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a65424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cbae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 100\n",
    "img_height = 300\n",
    "img_width = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67393cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the trained model and parameters\n",
    "trained_model = tf.keras.models.load_model(\"AccidentDetectionModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_vector = []\n",
    "\n",
    "n = 0\n",
    "for images, labels in testing_ds.take(1):\n",
    "    predictions = trained_model.predict(images)\n",
    "    predict_labels = []  # class_names\n",
    "    prdind = []  # index\n",
    "    \n",
    "    for p in predictions:\n",
    "        predict_labels.append(class_names[np.argmax(p)])\n",
    "        prdind.append(np.argmax(p))\n",
    "    \n",
    "    acc_vector = (np.array(prdlbl) == labels)\n",
    "    print('accuracy: ', np.mean(acc_vector))\n",
    "\n",
    "    if n==0:\n",
    "        print('predictions:', predictions)\n",
    "        print('predict_labels:', predict_labels)\n",
    "        print('prdind:', prdind)\n",
    "    \n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fec3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frame(img, model):\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    prediction=(model.predict(img_batch) < 0.5).astype(\"int32\")\n",
    "    print(\"prediction in predict_frame\", prediction)\n",
    "    if(prediction[0][0]==0):\n",
    "        return((\"Accident Detected\", 1))\n",
    "    else:\n",
    "        return((\"No Accident\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a82ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detection(image_path):\n",
    "    # 这里应替换为实际的调用逻辑\n",
    "    # 例如: 使用 subprocess.run([...], capture_output=True) 调用 detect.py 并解析输出\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47759a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b60c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def run_detection(image_path):\n",
    "    command = f\"python ./yolov5-master/detect.py --source {image_path}\"\n",
    "    result = subprocess.run(command, capture_output=True, text=True, shell=True)\n",
    "    print(\"result: \",result)\n",
    "    output = result.stdout\n",
    "    if \"person\" in output:\n",
    "        print(\"Person detected in the image.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No person detected in the image.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a5764b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  CompletedProcess(args='python ./yolov5-master/detect.py --source ./yolov5-master/data/images/zidane.jpg', returncode=1, stdout='', stderr='Traceback (most recent call last):\\n  File \"C:\\\\Users\\\\zhang\\\\Desktop\\\\projects\\\\MSE806ITS\\\\yolov5-master\\\\detect.py\", line 46, in <module>\\n    from ultralytics.utils.plotting import Annotator, colors, save_one_box\\nModuleNotFoundError: No module named \\'ultralytics\\'\\n')\n",
      "No person detected in the image.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_detection(\"./yolov5-master/data/images/zidane.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f64019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b84e336a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m images\u001b[38;5;241m=\u001b[39m[]  \u001b[38;5;66;03m# list of image frames\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pred_labels\u001b[38;5;241m=\u001b[39m[]  \u001b[38;5;66;03m# list of labels of each image frame, if collision detected\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m accident_detected \u001b[38;5;241m=\u001b[39m \u001b[43mfalse\u001b[49m\n\u001b[0;32m      7\u001b[0m cap\u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_collision/videoplayback_demo.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "# read the video stream\n",
    "images=[]  # list of image frames\n",
    "pred_labels=[]  # [(\"accident detected\", 1)] list of labels of each image frame, if collision detected\n",
    "human_detect_labels = []  # \n",
    "\n",
    "if_accident_detected = False\n",
    "sec_count = 0  #  count seconds after collision\n",
    "alert_threshold = 3  # alert after 3 seconds\n",
    "\n",
    "cap= cv2.VideoCapture('data_collision/videoplayback_demo.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # get the frames per second (fps) of the video stream\n",
    "frame_counts = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # total frame counts of the video stream\n",
    "print(\"fps, frame_counts: \", fps, frame_counts)\n",
    "\n",
    "\n",
    "for i in range(frame_counts):\n",
    "    ret, frame = cap.read()  # ret: if frame is successfully grabbed; frame_shape = (360.640,3)\n",
    "    if not ret:\n",
    "        print(\"Error: ret is false.\")\n",
    "        break\n",
    "    if i%fps==0:\n",
    "        print(i)\n",
    "        # print(\"ret, frame: \", ret, frame, frame.shape)\n",
    "        # resize the image frame from (360,640) to (300,300)\n",
    "        resized_frame=tf.keras.preprocessing.image.smart_resize(frame, (img_height, img_width), interpolation='bilinear')\n",
    "        images.append(frame)\n",
    "        collision_detect = predict_frame(resized_frame, trained_model)\n",
    "        pred_labels.append(collision_detect)\n",
    "        # accident detected == True\n",
    "        if collision_detect[1]:  \n",
    "            print(\"Accident Detected at \", i/fps, \" seconds\")\n",
    "            if_accident_detected = True\n",
    "            # save the frame per second for object detection\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmp_file:\n",
    "                cv2.imwrite(tmp_file.name, frame)\n",
    "                detected_labels = run_detection(tmp_file.name)\n",
    "        if if_accident_detected:\n",
    "            sec_count+=1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aec945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# 假设 run_detection 函数是 detect.py 修改后提供的，\n",
    "# 它接受图像路径作为输入，返回检测到的对象标签列表。\n",
    "# 如果 detect.py 不支持直接调用，请使用 subprocess 调用命令行并解析结果。\n",
    "def run_detection(image_path):\n",
    "    # 这里应替换为实际的调用逻辑\n",
    "    # 例如: 使用 subprocess.run([...], capture_output=True) 调用 detect.py 并解析输出\n",
    "    pass\n",
    "\n",
    "# 修改后的视频处理和检测逻辑\n",
    "cap = cv2.VideoCapture('data_collision/videoplayback_demo.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # 获取视频流的帧率\n",
    "frame_counts = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 获取视频流的总帧数\n",
    "accident_detected = False\n",
    "frames_after_accident = 0\n",
    "alert_threshold = 3 * fps  # 3秒内的帧数\n",
    "\n",
    "print(\"fps, frame_counts: \", fps, frame_counts)\n",
    "\n",
    "for i in range(frame_counts):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if i % fps == 0:  # 每秒处理一帧\n",
    "        # 假设的事故检测逻辑，collision_detect 应为布尔值，表示是否检测到事故\n",
    "        collision_detect = predict_frame(frame, trained_model)\n",
    "        \n",
    "        if collision_detect and not accident_detected:\n",
    "            print(\"Accident detected at frame\", i)\n",
    "            accident_detected = True\n",
    "            frames_after_accident = 0\n",
    "        \n",
    "        if accident_detected:\n",
    "            frames_after_accident += fps\n",
    "            if frames_after_accident <= alert_threshold:\n",
    "                # 保存当前帧到临时文件以进行检测\n",
    "                with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmp_file:\n",
    "                    cv2.imwrite(tmp_file.name, frame)\n",
    "                    detected_labels = run_detection(tmp_file.name)\n",
    "                \n",
    "                # 检查是否未检测到“人”\n",
    "                if \"person\" not in detected_labels:\n",
    "                    print(\"Alert: No person detected within 3 seconds after accident.\")\n",
    "                    # 发出警报的逻辑\n",
    "                    break  # 根据需要决定是否继续检查更多帧或中断循环\n",
    "\n",
    "cap.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
